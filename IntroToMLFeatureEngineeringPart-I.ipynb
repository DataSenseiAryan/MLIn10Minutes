{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this exercise, you will develop a baseline model for predicting if a customer will buy an app after clicking on an ad. With this baseline model, you'll be able to see how your feature engineering and selection efforts improve the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89489</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:13:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204158</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>2017-11-06 15:41:07</td>\n",
       "      <td>2017-11-07 08:17:19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3437</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 15:42:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167543</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:56:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147509</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:57:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71421</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>153</td>\n",
       "      <td>2017-11-06 16:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>76953</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 16:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>187909</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>477</td>\n",
       "      <td>2017-11-06 16:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>116779</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>150</td>\n",
       "      <td>2017-11-06 16:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47857</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>205</td>\n",
       "      <td>2017-11-06 16:00:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time      attributed_time  \\\n",
       "0   89489    3       1  13      379 2017-11-06 15:13:23                  NaN   \n",
       "1  204158   35       1  13       21 2017-11-06 15:41:07  2017-11-07 08:17:19   \n",
       "2    3437    6       1  13      459 2017-11-06 15:42:32                  NaN   \n",
       "3  167543    3       1  13      379 2017-11-06 15:56:17                  NaN   \n",
       "4  147509    3       1  13      379 2017-11-06 15:57:01                  NaN   \n",
       "5   71421   15       1  13      153 2017-11-06 16:00:00                  NaN   \n",
       "6   76953   14       1  13      379 2017-11-06 16:00:01                  NaN   \n",
       "7  187909    2       1  25      477 2017-11-06 16:00:01                  NaN   \n",
       "8  116779    1       1   8      150 2017-11-06 16:00:01                  NaN   \n",
       "9   47857    3       1  15      205 2017-11-06 16:00:01                  NaN   \n",
       "\n",
       "   is_attributed  \n",
       "0              0  \n",
       "1              1  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "5              0  \n",
       "6              0  \n",
       "7              0  \n",
       "8              0  \n",
       "9              0  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "click_data = pd.read_csv('../hitchhikersGuideToMachineLearning/train_sample.csv',parse_dates=['click_time'])\n",
    "click_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "The first thing you need to do is construct a baseline model. All new features, processing, encodings, and feature selection should improve upon this baseline model. First you need to do a bit of feature engineering before training the model itself.\n",
    "\n",
    "###  Features from timestamps\n",
    "From the timestamps, create features for the day, hour, minute and second. Store these as new integer columns `day`, `hour`, `minute`, and `second` in a new DataFrame `clicks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new columns for timestamp features day, hour, minute, and second\n",
    "clicks = click_data.copy()\n",
    "clicks['day'] = clicks['click_time'].dt.day.astype('uint8')\n",
    "# Fill in the rest\n",
    "clicks['hour'] = clicks['click_time'].dt.hour.astype('uint8')\n",
    "clicks['minute'] = clicks['click_time'].dt.minute.astype('uint8')\n",
    "clicks['second'] = clicks['click_time'].dt.second.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Categorical Data Encoding\n",
    "For each of the categorical features `['ip', 'app', 'device', 'os', 'channel']`, use scikit-learn's `LabelEncoder` to create new features in the `clicks` DataFrame. The new column names should be the original column name with `'_labels'` appended, like `ip_labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "\n",
    "# Create new columns in clicks using preprocessing.LabelEncoder()\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "for feature in cat_features:\n",
    "    encoded = encoder.fit_transform(clicks[feature])\n",
    "    clicks[feature + \"_labels\"] = encoded\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, validation, and test sets\n",
    "With our baseline features ready, we need to split our data into training and validation sets. We should also hold out a test set to measure the final accuracy of the model.\n",
    "\n",
    "### Train/test splits with time series data\n",
    "This is time series data. Are they any special considerations when creating train/test splits for time series.\n",
    "I have explained Time series Cross Validation in this article link and Data Leakage caused by them i this Article link\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['day', 'hour', 'minute', 'second', \n",
    "                'ip_labels', 'app_labels', 'device_labels',\n",
    "                'os_labels', 'channel_labels']\n",
    "\n",
    "valid_fraction = 0.1\n",
    "clicks_srt = clicks.sort_values('click_time')\n",
    "valid_rows = int(len(clicks_srt) * valid_fraction)\n",
    "train = clicks_srt[:-valid_rows * 2]\n",
    "valid = clicks_srt[-valid_rows * 2:-valid_rows]\n",
    "test = clicks_srt[-valid_rows:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with LightGBM\n",
    "\n",
    "Now we can create LightGBM dataset objects for each of the smaller datasets and train the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's auc: 0.948979\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[2]\tvalid_0's auc: 0.949235\n",
      "[3]\tvalid_0's auc: 0.950126\n",
      "[4]\tvalid_0's auc: 0.950072\n",
      "[5]\tvalid_0's auc: 0.950536\n",
      "[6]\tvalid_0's auc: 0.950943\n",
      "[7]\tvalid_0's auc: 0.951453\n",
      "[8]\tvalid_0's auc: 0.951518\n",
      "[9]\tvalid_0's auc: 0.952385\n",
      "[10]\tvalid_0's auc: 0.952434\n",
      "[11]\tvalid_0's auc: 0.952465\n",
      "[12]\tvalid_0's auc: 0.952638\n",
      "[13]\tvalid_0's auc: 0.95266\n",
      "[14]\tvalid_0's auc: 0.952766\n",
      "[15]\tvalid_0's auc: 0.953203\n",
      "[16]\tvalid_0's auc: 0.953503\n",
      "[17]\tvalid_0's auc: 0.953793\n",
      "[18]\tvalid_0's auc: 0.953966\n",
      "[19]\tvalid_0's auc: 0.954184\n",
      "[20]\tvalid_0's auc: 0.9543\n",
      "[21]\tvalid_0's auc: 0.954305\n",
      "[22]\tvalid_0's auc: 0.954536\n",
      "[23]\tvalid_0's auc: 0.954748\n",
      "[24]\tvalid_0's auc: 0.955142\n",
      "[25]\tvalid_0's auc: 0.955493\n",
      "[26]\tvalid_0's auc: 0.955611\n",
      "[27]\tvalid_0's auc: 0.955708\n",
      "[28]\tvalid_0's auc: 0.955795\n",
      "[29]\tvalid_0's auc: 0.956172\n",
      "[30]\tvalid_0's auc: 0.95623\n",
      "[31]\tvalid_0's auc: 0.956477\n",
      "[32]\tvalid_0's auc: 0.956606\n",
      "[33]\tvalid_0's auc: 0.956864\n",
      "[34]\tvalid_0's auc: 0.957204\n",
      "[35]\tvalid_0's auc: 0.957327\n",
      "[36]\tvalid_0's auc: 0.957408\n",
      "[37]\tvalid_0's auc: 0.957524\n",
      "[38]\tvalid_0's auc: 0.957659\n",
      "[39]\tvalid_0's auc: 0.957846\n",
      "[40]\tvalid_0's auc: 0.958042\n",
      "[41]\tvalid_0's auc: 0.958146\n",
      "[42]\tvalid_0's auc: 0.958181\n",
      "[43]\tvalid_0's auc: 0.958285\n",
      "[44]\tvalid_0's auc: 0.958433\n",
      "[45]\tvalid_0's auc: 0.95854\n",
      "[46]\tvalid_0's auc: 0.958625\n",
      "[47]\tvalid_0's auc: 0.958756\n",
      "[48]\tvalid_0's auc: 0.958863\n",
      "[49]\tvalid_0's auc: 0.958938\n",
      "[50]\tvalid_0's auc: 0.959046\n",
      "[51]\tvalid_0's auc: 0.95908\n",
      "[52]\tvalid_0's auc: 0.959147\n",
      "[53]\tvalid_0's auc: 0.9592\n",
      "[54]\tvalid_0's auc: 0.959259\n",
      "[55]\tvalid_0's auc: 0.959311\n",
      "[56]\tvalid_0's auc: 0.959324\n",
      "[57]\tvalid_0's auc: 0.959348\n",
      "[58]\tvalid_0's auc: 0.959435\n",
      "[59]\tvalid_0's auc: 0.959463\n",
      "[60]\tvalid_0's auc: 0.95949\n",
      "[61]\tvalid_0's auc: 0.959562\n",
      "[62]\tvalid_0's auc: 0.959721\n",
      "[63]\tvalid_0's auc: 0.959729\n",
      "[64]\tvalid_0's auc: 0.959773\n",
      "[65]\tvalid_0's auc: 0.959809\n",
      "[66]\tvalid_0's auc: 0.959868\n",
      "[67]\tvalid_0's auc: 0.959921\n",
      "[68]\tvalid_0's auc: 0.959994\n",
      "[69]\tvalid_0's auc: 0.960065\n",
      "[70]\tvalid_0's auc: 0.96011\n",
      "[71]\tvalid_0's auc: 0.960133\n",
      "[72]\tvalid_0's auc: 0.960275\n",
      "[73]\tvalid_0's auc: 0.960299\n",
      "[74]\tvalid_0's auc: 0.960336\n",
      "[75]\tvalid_0's auc: 0.960365\n",
      "[76]\tvalid_0's auc: 0.960411\n",
      "[77]\tvalid_0's auc: 0.960488\n",
      "[78]\tvalid_0's auc: 0.960523\n",
      "[79]\tvalid_0's auc: 0.960563\n",
      "[80]\tvalid_0's auc: 0.960624\n",
      "[81]\tvalid_0's auc: 0.960665\n",
      "[82]\tvalid_0's auc: 0.960724\n",
      "[83]\tvalid_0's auc: 0.960724\n",
      "[84]\tvalid_0's auc: 0.960751\n",
      "[85]\tvalid_0's auc: 0.960799\n",
      "[86]\tvalid_0's auc: 0.960853\n",
      "[87]\tvalid_0's auc: 0.960876\n",
      "[88]\tvalid_0's auc: 0.960934\n",
      "[89]\tvalid_0's auc: 0.961012\n",
      "[90]\tvalid_0's auc: 0.961012\n",
      "[91]\tvalid_0's auc: 0.961065\n",
      "[92]\tvalid_0's auc: 0.961095\n",
      "[93]\tvalid_0's auc: 0.961131\n",
      "[94]\tvalid_0's auc: 0.961136\n",
      "[95]\tvalid_0's auc: 0.961155\n",
      "[96]\tvalid_0's auc: 0.961191\n",
      "[97]\tvalid_0's auc: 0.961189\n",
      "[98]\tvalid_0's auc: 0.961189\n",
      "[99]\tvalid_0's auc: 0.961224\n",
      "[100]\tvalid_0's auc: 0.961228\n",
      "[101]\tvalid_0's auc: 0.96125\n",
      "[102]\tvalid_0's auc: 0.961259\n",
      "[103]\tvalid_0's auc: 0.961289\n",
      "[104]\tvalid_0's auc: 0.961309\n",
      "[105]\tvalid_0's auc: 0.961309\n",
      "[106]\tvalid_0's auc: 0.96134\n",
      "[107]\tvalid_0's auc: 0.961373\n",
      "[108]\tvalid_0's auc: 0.961382\n",
      "[109]\tvalid_0's auc: 0.961391\n",
      "[110]\tvalid_0's auc: 0.961402\n",
      "[111]\tvalid_0's auc: 0.961449\n",
      "[112]\tvalid_0's auc: 0.96145\n",
      "[113]\tvalid_0's auc: 0.961482\n",
      "[114]\tvalid_0's auc: 0.961481\n",
      "[115]\tvalid_0's auc: 0.961492\n",
      "[116]\tvalid_0's auc: 0.961513\n",
      "[117]\tvalid_0's auc: 0.961531\n",
      "[118]\tvalid_0's auc: 0.961539\n",
      "[119]\tvalid_0's auc: 0.961563\n",
      "[120]\tvalid_0's auc: 0.961563\n",
      "[121]\tvalid_0's auc: 0.961568\n",
      "[122]\tvalid_0's auc: 0.961588\n",
      "[123]\tvalid_0's auc: 0.961599\n",
      "[124]\tvalid_0's auc: 0.961605\n",
      "[125]\tvalid_0's auc: 0.961605\n",
      "[126]\tvalid_0's auc: 0.96161\n",
      "[127]\tvalid_0's auc: 0.961626\n",
      "[128]\tvalid_0's auc: 0.961626\n",
      "[129]\tvalid_0's auc: 0.96163\n",
      "[130]\tvalid_0's auc: 0.961646\n",
      "[131]\tvalid_0's auc: 0.961678\n",
      "[132]\tvalid_0's auc: 0.961672\n",
      "[133]\tvalid_0's auc: 0.961673\n",
      "[134]\tvalid_0's auc: 0.96171\n",
      "[135]\tvalid_0's auc: 0.96171\n",
      "[136]\tvalid_0's auc: 0.961724\n",
      "[137]\tvalid_0's auc: 0.961723\n",
      "[138]\tvalid_0's auc: 0.961726\n",
      "[139]\tvalid_0's auc: 0.961731\n",
      "[140]\tvalid_0's auc: 0.961736\n",
      "[141]\tvalid_0's auc: 0.961751\n",
      "[142]\tvalid_0's auc: 0.961759\n",
      "[143]\tvalid_0's auc: 0.961777\n",
      "[144]\tvalid_0's auc: 0.961777\n",
      "[145]\tvalid_0's auc: 0.961779\n",
      "[146]\tvalid_0's auc: 0.961782\n",
      "[147]\tvalid_0's auc: 0.961782\n",
      "[148]\tvalid_0's auc: 0.961796\n",
      "[149]\tvalid_0's auc: 0.961799\n",
      "[150]\tvalid_0's auc: 0.961806\n",
      "[151]\tvalid_0's auc: 0.961804\n",
      "[152]\tvalid_0's auc: 0.961805\n",
      "[153]\tvalid_0's auc: 0.961794\n",
      "[154]\tvalid_0's auc: 0.961802\n",
      "[155]\tvalid_0's auc: 0.961805\n",
      "[156]\tvalid_0's auc: 0.961821\n",
      "[157]\tvalid_0's auc: 0.961853\n",
      "[158]\tvalid_0's auc: 0.96187\n",
      "[159]\tvalid_0's auc: 0.961875\n",
      "[160]\tvalid_0's auc: 0.961877\n",
      "[161]\tvalid_0's auc: 0.961889\n",
      "[162]\tvalid_0's auc: 0.961894\n",
      "[163]\tvalid_0's auc: 0.961898\n",
      "[164]\tvalid_0's auc: 0.961901\n",
      "[165]\tvalid_0's auc: 0.961911\n",
      "[166]\tvalid_0's auc: 0.961911\n",
      "[167]\tvalid_0's auc: 0.961915\n",
      "[168]\tvalid_0's auc: 0.961925\n",
      "[169]\tvalid_0's auc: 0.961925\n",
      "[170]\tvalid_0's auc: 0.961929\n",
      "[171]\tvalid_0's auc: 0.961949\n",
      "[172]\tvalid_0's auc: 0.961945\n",
      "[173]\tvalid_0's auc: 0.961945\n",
      "[174]\tvalid_0's auc: 0.961944\n",
      "[175]\tvalid_0's auc: 0.961946\n",
      "[176]\tvalid_0's auc: 0.961952\n",
      "[177]\tvalid_0's auc: 0.961956\n",
      "[178]\tvalid_0's auc: 0.961958\n",
      "[179]\tvalid_0's auc: 0.961971\n",
      "[180]\tvalid_0's auc: 0.961998\n",
      "[181]\tvalid_0's auc: 0.961998\n",
      "[182]\tvalid_0's auc: 0.962014\n",
      "[183]\tvalid_0's auc: 0.962018\n",
      "[184]\tvalid_0's auc: 0.962016\n",
      "[185]\tvalid_0's auc: 0.962022\n",
      "[186]\tvalid_0's auc: 0.962031\n",
      "[187]\tvalid_0's auc: 0.96203\n",
      "[188]\tvalid_0's auc: 0.962021\n",
      "[189]\tvalid_0's auc: 0.962021\n",
      "[190]\tvalid_0's auc: 0.962022\n",
      "[191]\tvalid_0's auc: 0.962026\n",
      "[192]\tvalid_0's auc: 0.962038\n",
      "[193]\tvalid_0's auc: 0.962042\n",
      "[194]\tvalid_0's auc: 0.962041\n",
      "[195]\tvalid_0's auc: 0.962035\n",
      "[196]\tvalid_0's auc: 0.962037\n",
      "[197]\tvalid_0's auc: 0.962048\n",
      "[198]\tvalid_0's auc: 0.962054\n",
      "[199]\tvalid_0's auc: 0.962052\n",
      "[200]\tvalid_0's auc: 0.962054\n",
      "[201]\tvalid_0's auc: 0.962041\n",
      "[202]\tvalid_0's auc: 0.962041\n",
      "[203]\tvalid_0's auc: 0.962052\n",
      "[204]\tvalid_0's auc: 0.962051\n",
      "[205]\tvalid_0's auc: 0.962056\n",
      "[206]\tvalid_0's auc: 0.962056\n",
      "[207]\tvalid_0's auc: 0.962069\n",
      "[208]\tvalid_0's auc: 0.962072\n",
      "[209]\tvalid_0's auc: 0.962072\n",
      "[210]\tvalid_0's auc: 0.962062\n",
      "[211]\tvalid_0's auc: 0.962064\n",
      "[212]\tvalid_0's auc: 0.962066\n",
      "[213]\tvalid_0's auc: 0.962066\n",
      "[214]\tvalid_0's auc: 0.962066\n",
      "[215]\tvalid_0's auc: 0.962064\n",
      "[216]\tvalid_0's auc: 0.96206\n",
      "[217]\tvalid_0's auc: 0.962059\n",
      "[218]\tvalid_0's auc: 0.962059\n",
      "Early stopping, best iteration is:\n",
      "[208]\tvalid_0's auc: 0.962072\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "dtest = lgb.Dataset(test[feature_cols], label=test['is_attributed'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "Finally, with the model trained, I'll evaluate it's performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9726727334566094\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "ypred = bst.predict(test[feature_cols])\n",
    "score = metrics.roc_auc_score(test['is_attributed'], ypred)\n",
    "print(f\"Test score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diving Deeper in Categorical Encoding\n",
    "\n",
    "In this exercise you'll apply more advanced encodings to encode the categorical variables ito improve your classifier model. The encodings you will implement are:\n",
    "\n",
    "- Count Encoding\n",
    "- Target Encoding\n",
    "- Leave-one-out Encoding\n",
    "- CatBoost Encoding\n",
    "- Feature embedding with SVD \n",
    "\n",
    "You'll refit the classifier after each encoding to check its performance on hold-out data. First, run the next cell to repeat the work you did in the last exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be traning and testing alot lests make some functions! Wherever you to use a piece of code more thn thrice make a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    \"\"\" Splits a dataframe into train, validation, and test sets. First, orders by \n",
    "        the column 'click_time'. Set the size of the validation and test sets with\n",
    "        the valid_fraction keyword argument.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = ce.CountEncoder(cols = cat_features)\n",
    "\n",
    "\n",
    "\n",
    "# Learn encoding from the training set\n",
    "count_encoded = count_enc.fit(train[cat_features])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_count` as a suffix to the new columns\n",
    "train_encoded = train.join(count_enc.transform(train[cat_features]).add_suffix('_count'))\n",
    "\n",
    "valid_encoded = valid.join(count_enc.transform(valid[cat_features]).add_suffix('_count'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9649930021641716\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the encoded datasets\n",
    "# This can take around 30 seconds to complete\n",
    "_ = train_model(train_encoded, valid_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the target encoder. You can find this easily by using tab completion.\n",
    "# Start typing ce. the press Tab to bring up a list of classes and functions.\n",
    "target_enc = ce.TargetEncoder(cols=cat_features)\n",
    "\n",
    "\n",
    "# Learn encoding from the training set. Use the 'is_attributed' column as the target.\n",
    "target_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_target` as a suffix to the new columns\n",
    "train_encoded = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))\n",
    "\n",
    "valid_encoded = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9540530347873288\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train_encoded, valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " leaving ip out of the encoded features and retrain the model with target encoding again. You should find that the score increases and is above the baseline score! \n",
    " As IP addresses are too many target encoding will add noise only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IP address of an internet transaction\n",
    "is another example of a large categorical variable. They are categorical variables\n",
    "because, even though user IDs and IP addresses are numeric, their magnitude is\n",
    "usually not relevant to the task at hand. For instance, the IP address might be\n",
    "relevant when doing fraud detection on individual transactions—some IP\n",
    "addresses or subnets may generate more fraudulent transactions than others. But\n",
    "a subnet of 164.203.x.x is not inherently more fraudulent than 164.202.x.x; the\n",
    "numeric value of the subnet does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "cat_features = ['app', 'device', 'os', 'channel']\n",
    "\n",
    "# Create the CatBoost encoder\n",
    "cb_enc = target_enc = ce.CatBoostEncoder(cols=cat_features , random_state=7)\n",
    "\n",
    "# Learn encoding from the training set\n",
    "cb_enc.fit(train[cat_features], train['is_attributed'])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_cb` as a suffix to the new columns\n",
    "train_encoded = train.join(cb_enc.transform(train[cat_features]).add_suffix('_cb'))\n",
    "valid_encoded = valid.join(cb_enc.transform(valid[cat_features]).add_suffix('_cb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9626909733248851\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train_encoded, valid_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost works best so we will keep it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoded = cb_enc.transform(clicks[cat_features])\n",
    "for col in encoded:\n",
    "    clicks.insert(len(clicks.columns), col + '_cb', encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>second</th>\n",
       "      <th>ip_labels</th>\n",
       "      <th>app_labels</th>\n",
       "      <th>device_labels</th>\n",
       "      <th>os_labels</th>\n",
       "      <th>channel_labels</th>\n",
       "      <th>app_cb</th>\n",
       "      <th>device_cb</th>\n",
       "      <th>os_cb</th>\n",
       "      <th>channel_cb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89489</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:13:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>27226</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.152087</td>\n",
       "      <td>0.138712</td>\n",
       "      <td>0.034049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204158</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>2017-11-06 15:41:07</td>\n",
       "      <td>2017-11-07 08:17:19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>110007</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.995828</td>\n",
       "      <td>0.152087</td>\n",
       "      <td>0.138712</td>\n",
       "      <td>0.950244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3437</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 15:42:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>32</td>\n",
       "      <td>1047</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>157</td>\n",
       "      <td>0.009261</td>\n",
       "      <td>0.152087</td>\n",
       "      <td>0.138712</td>\n",
       "      <td>0.019384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167543</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:56:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>76270</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.152087</td>\n",
       "      <td>0.138712</td>\n",
       "      <td>0.034049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147509</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:57:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>57862</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>120</td>\n",
       "      <td>0.028329</td>\n",
       "      <td>0.152087</td>\n",
       "      <td>0.138712</td>\n",
       "      <td>0.034049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time      attributed_time  \\\n",
       "0   89489    3       1  13      379 2017-11-06 15:13:23                  NaN   \n",
       "1  204158   35       1  13       21 2017-11-06 15:41:07  2017-11-07 08:17:19   \n",
       "2    3437    6       1  13      459 2017-11-06 15:42:32                  NaN   \n",
       "3  167543    3       1  13      379 2017-11-06 15:56:17                  NaN   \n",
       "4  147509    3       1  13      379 2017-11-06 15:57:01                  NaN   \n",
       "\n",
       "   is_attributed  day  hour  ...  second  ip_labels  app_labels  \\\n",
       "0              0    6    15  ...      23      27226           3   \n",
       "1              1    6    15  ...       7     110007          35   \n",
       "2              0    6    15  ...      32       1047           6   \n",
       "3              0    6    15  ...      17      76270           3   \n",
       "4              0    6    15  ...       1      57862           3   \n",
       "\n",
       "   device_labels  os_labels  channel_labels    app_cb  device_cb     os_cb  \\\n",
       "0              1         13             120  0.028329   0.152087  0.138712   \n",
       "1              1         13              10  0.995828   0.152087  0.138712   \n",
       "2              1         13             157  0.009261   0.152087  0.138712   \n",
       "3              1         13             120  0.028329   0.152087  0.138712   \n",
       "4              1         13             120  0.028329   0.152087  0.138712   \n",
       "\n",
       "   channel_cb  \n",
       "0    0.034049  \n",
       "1    0.950244  \n",
       "2    0.019384  \n",
       "3    0.034049  \n",
       "4    0.034049  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add interaction features\n",
    "\n",
    "Here you'll add interaction features for each pair of categorical features (ip, app, device, os, channel). The easiest way to iterate through the pairs of features is with `itertools.combinations`. For each new column, join the values as strings with an underscore, so 13 and 47 would become `\"13_47\"`. As you add the new columns to the dataset, be sure to label encode the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "interactions = pd.DataFrame(index=clicks.index)\n",
    "\n",
    "# Iterate through each pair of features, combine them into interaction features\n",
    "for col1 ,col2 in  itertools.combinations(cat_features,2):\n",
    "    newcolname = col1 + \"_\" + col2 \n",
    "    new_values = clicks[col1].map(str) + \"_\" + clicks[col2].map(str)\n",
    "\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    interactions[newcolname] = encoder.fit_transform(new_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score with interactions\n",
      "Training model!\n",
      "Validation AUC score: 0.9627791479263492\n"
     ]
    }
   ],
   "source": [
    "clicks = clicks.join(interactions)\n",
    "print(\"Score with interactions\")\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating numerical features\n",
    "\n",
    "Adding interactions is a quick way to create more categorical features from the data. It's also effective to create new numerical features, you'll typically get a lot of improvement in the model. This takes a bit of brainstorming and experimentation to find features that work well.\n",
    "\n",
    "For these exercises I'm going to have you implement functions that operate on Pandas Series. It can take multiple minutes to run these functions on the entire data set so instead I'll provide feedback by running your function on a smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Number of events in the past six hours\n",
    "\n",
    "The first feature you'll be creating is the number of events from the same IP in the last six hours. It's likely that someone who is visiting often will download the app.\n",
    "\n",
    "Implement a function `count_past_events` that takes a Series of click times (timestamps) and returns another Series with the number of events in the last hour. **Tip:** The `rolling` method is useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_past_events(series, time_window='6H'):\n",
    "    series = pd.Series(series.index, index=series)\n",
    "    past_events = series.rolling(time_window).count() - 1\n",
    "    return past_events\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_events = count_past_events(clicks['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks['ip_past_6hr_counts'] = past_events.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>app</th>\n",
       "      <th>device</th>\n",
       "      <th>os</th>\n",
       "      <th>channel</th>\n",
       "      <th>click_time</th>\n",
       "      <th>attributed_time</th>\n",
       "      <th>is_attributed</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>ip_device</th>\n",
       "      <th>ip_os</th>\n",
       "      <th>ip_channel</th>\n",
       "      <th>app_device</th>\n",
       "      <th>app_os</th>\n",
       "      <th>app_channel</th>\n",
       "      <th>device_os</th>\n",
       "      <th>device_channel</th>\n",
       "      <th>os_channel</th>\n",
       "      <th>ip_past_6hr_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89489</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:13:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>327558</td>\n",
       "      <td>844429</td>\n",
       "      <td>1204595</td>\n",
       "      <td>3543</td>\n",
       "      <td>3973</td>\n",
       "      <td>621</td>\n",
       "      <td>795</td>\n",
       "      <td>1534</td>\n",
       "      <td>1123</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>204158</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>2017-11-06 15:41:07</td>\n",
       "      <td>2017-11-07 08:17:19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>110989</td>\n",
       "      <td>324393</td>\n",
       "      <td>473773</td>\n",
       "      <td>3486</td>\n",
       "      <td>3715</td>\n",
       "      <td>561</td>\n",
       "      <td>795</td>\n",
       "      <td>1465</td>\n",
       "      <td>1059</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3437</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>459</td>\n",
       "      <td>2017-11-06 15:42:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>264762</td>\n",
       "      <td>590544</td>\n",
       "      <td>795240</td>\n",
       "      <td>4180</td>\n",
       "      <td>5063</td>\n",
       "      <td>777</td>\n",
       "      <td>795</td>\n",
       "      <td>1570</td>\n",
       "      <td>1154</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>167543</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:56:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>67781</td>\n",
       "      <td>221287</td>\n",
       "      <td>333763</td>\n",
       "      <td>3543</td>\n",
       "      <td>3973</td>\n",
       "      <td>621</td>\n",
       "      <td>795</td>\n",
       "      <td>1534</td>\n",
       "      <td>1123</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>147509</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>379</td>\n",
       "      <td>2017-11-06 15:57:01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>44449</td>\n",
       "      <td>166639</td>\n",
       "      <td>260146</td>\n",
       "      <td>3543</td>\n",
       "      <td>3973</td>\n",
       "      <td>621</td>\n",
       "      <td>795</td>\n",
       "      <td>1534</td>\n",
       "      <td>1123</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ip  app  device  os  channel          click_time      attributed_time  \\\n",
       "0   89489    3       1  13      379 2017-11-06 15:13:23                  NaN   \n",
       "1  204158   35       1  13       21 2017-11-06 15:41:07  2017-11-07 08:17:19   \n",
       "2    3437    6       1  13      459 2017-11-06 15:42:32                  NaN   \n",
       "3  167543    3       1  13      379 2017-11-06 15:56:17                  NaN   \n",
       "4  147509    3       1  13      379 2017-11-06 15:57:01                  NaN   \n",
       "\n",
       "   is_attributed  day  hour  ...  ip_device   ip_os  ip_channel  app_device  \\\n",
       "0              0    6    15  ...     327558  844429     1204595        3543   \n",
       "1              1    6    15  ...     110989  324393      473773        3486   \n",
       "2              0    6    15  ...     264762  590544      795240        4180   \n",
       "3              0    6    15  ...      67781  221287      333763        3543   \n",
       "4              0    6    15  ...      44449  166639      260146        3543   \n",
       "\n",
       "   app_os  app_channel  device_os  device_channel  os_channel  \\\n",
       "0    3973          621        795            1534        1123   \n",
       "1    3715          561        795            1465        1059   \n",
       "2    5063          777        795            1570        1154   \n",
       "3    3973          621        795            1534        1123   \n",
       "4    3973          621        795            1534        1123   \n",
       "\n",
       "   ip_past_6hr_counts  \n",
       "0                 0.0  \n",
       "1                 1.0  \n",
       "2                 2.0  \n",
       "3                 3.0  \n",
       "4                 4.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9623695834546713\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Time since last event\n",
    "\n",
    "Implement a function `time_diff` that calculates the time since the last event in seconds from a Series of timestamps. This will be ran like so:\n",
    "\n",
    "```python\n",
    "timedeltas = clicks.groupby('ip')['click_time'].transform(time_diff)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_diff(series):\n",
    "    \"\"\" Returns a series with the time since the last timestamp in seconds \"\"\"\n",
    "    return series.diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_events = time_diff(clicks['click_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      nan, 1.664e+03, 8.500e+01, ..., 1.000e+00, 0.000e+00,\n",
       "       0.000e+00])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks['past_events_6hr'] = past_events.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ip', 'app', 'device', 'os', 'channel', 'click_time', 'attributed_time',\n",
       "       'is_attributed', 'day', 'hour', 'minute', 'second', 'ip_labels',\n",
       "       'app_labels', 'device_labels', 'os_labels', 'channel_labels', 'app_cb',\n",
       "       'device_cb', 'os_cb', 'channel_cb', 'ip_app', 'ip_device', 'ip_os',\n",
       "       'ip_channel', 'app_device', 'app_os', 'app_channel', 'device_os',\n",
       "       'device_channel', 'os_channel', 'ip_past_6hr_counts',\n",
       "       'past_events_6hr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "Validation AUC score: 0.9624544752151282\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "_ = train_model(train, valid, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Which data to use for feature selection?\n",
    "\n",
    "Since many feature selection methods require calculating statistics from the dataset, should you use all the data for feature selection?\n",
    "\n",
    "Run the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
