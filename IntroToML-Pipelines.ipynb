{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelining In Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save filepath to variable for easier access\n",
    "melbourne_file_path = '../hitchhikersGuideToMachineLearning/home-data-for-ml-course/train.csv'\n",
    "# read the data and store data in DataFrame titled melbourne_data\n",
    "train_data = pd.read_csv(melbourne_file_path) \n",
    "# print a summary of the data in Melbourne data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of directly attacking the dataset I will cover the capabilities of Piplelining library from scikit-learn package!\n",
    "- Pipelines and composite estimators\n",
    "- Pipeline: chaining estimators\n",
    "- Transforming target in regression\n",
    "- FeatureUnion: composite feature spaces\n",
    "- ColumnTransformer for heterogeneous data\n",
    "\n",
    "Especially FetureUnion and ColumnTransformers are important.\n",
    "Also I will demontrate how to incorporate custom techniques and tricks into piplines.\n",
    "We will use few of this tricks on our dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning and preprocessing are a crucial step in the machine learning project.\n",
    "Whenever new data points are added to the existing data, we need to perform the same preprocessing steps again before we can use the machine learning model to make predictions. This becomes a tedious and time-consuming process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternate to this is creating a machine learning pipeline that remembers the complete set of preprocessing steps in the exact same order. So that whenever any new data point is introduced, the machine learning pipeline performs the steps as defined and uses the machine learning model to predict the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up a machine learning algorithm involves more than the algorithm itself. You need to preprocess the data in order for it to fit the algorithm. It's this preprocessing pipeline that often requires a lot of work. Building a flexible pipeline is key. Here's how you can build it in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a pipeline?\n",
    "A pipeline in sklearn is a set of chained algorithms to extract features, preprocess them and then train or use a machine learning algorith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and split the data\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size= 0.2,random_state=42 )\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construction\n",
    "The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('minmax', MinMaxScaler()),('lr', LogisticRegression(C=1))]\n",
    "pipe = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('minmax', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression pipeline test accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "score = pipe.score(X_test, y_test)\n",
    "print('Logistic Regression pipeline test accuracy: %.3f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility function make_pipeline is a shorthand for constructing pipelines; it takes a variable number of estimators and returns a pipeline, filling in the names automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=10, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipe2= make_pipeline(MinMaxScaler(), LogisticRegression(C=10))\n",
    "pipe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accessing steps\n",
    "The estimators of a pipeline are stored as a list in the steps attribute, but can be accessed by index or name by indexing (with [idx]) the Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('minmax', MinMaxScaler(copy=True, feature_range=(0, 1)))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe['minmax']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline’s named_steps attribute allows accessing steps by name with tab completion in interactive environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=(0, 1))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps.minmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sub-pipeline can also be extracted using the slicing notation commonly used for Python Sequences such as lists or strings (although only a step of 1 is permitted). This is convenient for performing only some of the transformations (or their inverse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('minmaxscaler',\n",
       "                 MinMaxScaler(copy=True, feature_range=(0, 1)))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Nested parameters\n",
    "Parameters of the estimators in the pipeline can be accessed using the <estimator>__<parameter> syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('minmax', MinMaxScaler(copy=True, feature_range=(0, 1))),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=2, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(lr__C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly important for doing grid searches!\n",
    "\n",
    "Individual steps may also be replaced as parameters, and non-final steps may be ignored by setting them to 'passthrough'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings='ignore'\n",
    "param_grid = dict(minmax=['passthrough'],lr__C=[1, 2, 3])\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "gd=grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('minmax', 'passthrough'),\n",
       "                ('lr',\n",
       "                 LogisticRegression(C=1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gd.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = pipe.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore a regression dataset so that i can drill a few more useful points in your skull!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [('minmax', MinMaxScaler()),('lr', LinearRegression())]\n",
    "raw_Y_regr = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.64\n"
     ]
    }
   ],
   "source": [
    "raw_Y_regr.fit(X_train, y_train)\n",
    "print('R2 score: {0:.2f}'.format(raw_Y_regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I can transform the input features and do all kinds of preprocessing , but what if i want to transform \n",
    "Y. In many regression tasks you may have to transform Y while feeding the input to the model.For example taing log of Y. \n",
    "But when we are predicting we ill have to scale it back to original unit basically we will have apply inverse of the transformation on predicted Y.\n",
    "All this can be done in PIpeline very easily!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "regressor = LinearRegression()\n",
    "regr = TransformedTargetRegressor(regressor=regressor,transformer=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:2357: UserWarning: n_quantiles (1000) is greater than the total number of samples (379). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n"
     ]
    }
   ],
   "source": [
    "regr.fit(X_train, y_train)\n",
    "print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple transformations, instead of a Transformer object, a pair of functions can be passed, defining the transformation and its inverse mapping. It means you can make your custom transformers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "     return np.log(x)\n",
    "def inverse_func(x):\n",
    "     return np.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = TransformedTargetRegressor(regressor=regressor,func=func,inverse_func=inverse_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformedTargetRegressor(check_inverse=True,\n",
       "                           func=<function func at 0x7fa075054400>,\n",
       "                           inverse_func=<function inverse_func at 0x7fa0265b9048>,\n",
       "                           regressor=LinearRegression(copy_X=True,\n",
       "                                                      fit_intercept=True,\n",
       "                                                      n_jobs=None,\n",
       "                                                      normalize=False),\n",
       "                           transformer=None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.65\n"
     ]
    }
   ],
   "source": [
    "print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FeaturUnions\n",
    "This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "estimators = [('linear_pca', PCA()), ('select_k_best', SelectKBest(k=10))]\n",
    "combined_features = FeatureUnion(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From PCA I am selecting 8 and from select K best I am slecting 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined space has 23 features\n"
     ]
    }
   ],
   "source": [
    "# Use combined features to transform dataset:\n",
    "X_features = combined_features.fit(X, y).transform(X)\n",
    "print(\"Combined space has\", X_features.shape[1], \"features\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"lr\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=5, score=0.246, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=5, score=0.660, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=5, score=0.197, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=5, score=-0.018, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=5, score=-0.243, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=8, score=0.633, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=8, score=0.733, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=8, score=0.673, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=8, score=0.065, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=4, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=4, features__select_k_best__k=8, score=-0.369, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=5, score=0.476, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=5, score=0.732, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=5, score=0.663, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=5, score=0.052, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=5 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=5, score=-0.368, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=8, score=0.639, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=8, score=0.714, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=8, score=0.587, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=8, score=0.079, total=   0.0s\n",
      "[CV] features__linear_pca__n_components=6, features__select_k_best__k=8 \n",
      "[CV]  features__linear_pca__n_components=6, features__select_k_best__k=8, score=0.037, total=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('features',\n",
       "                                        FeatureUnion(n_jobs=None,\n",
       "                                                     transformer_list=[('linear_pca',\n",
       "                                                                        PCA(copy=True,\n",
       "                                                                            iterated_power='auto',\n",
       "                                                                            n_components=None,\n",
       "                                                                            random_state=None,\n",
       "                                                                            svd_solver='auto',\n",
       "                                                                            tol=0.0,\n",
       "                                                                            whiten=False)),\n",
       "                                                                       ('select_k_best',\n",
       "                                                                        SelectKBest(k=10,\n",
       "                                                                                    score_func=<function f_classif at 0x7fa027460730>))],\n",
       "                                                     transformer_weights=None,\n",
       "                                                     verbose=False)),\n",
       "                                       ('lr',\n",
       "                                        LinearRegression(copy_X=True,\n",
       "                                                         fit_intercept=True,\n",
       "                                                         n_jobs=None,\n",
       "                                                         normalize=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'features__linear_pca__n_components': [4, 6],\n",
       "                         'features__select_k_best__k': [5, 8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = dict(features__linear_pca__n_components=[4, 6],\n",
    "                  features__select_k_best__k=[5, 8])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, verbose=10)\n",
    "grid_search.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(memory=None,\n",
      "         steps=[('features',\n",
      "                 FeatureUnion(n_jobs=None,\n",
      "                              transformer_list=[('linear_pca',\n",
      "                                                 PCA(copy=True,\n",
      "                                                     iterated_power='auto',\n",
      "                                                     n_components=6,\n",
      "                                                     random_state=None,\n",
      "                                                     svd_solver='auto', tol=0.0,\n",
      "                                                     whiten=False)),\n",
      "                                                ('select_k_best',\n",
      "                                                 SelectKBest(k=8,\n",
      "                                                             score_func=<function f_classif at 0x7fa027460730>))],\n",
      "                              transformer_weights=None, verbose=False)),\n",
      "                ('lr',\n",
      "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "                                  normalize=False))],\n",
      "         verbose=False)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ColumnTransformer for heterogeneous data\n",
    "Many datasets contain features of different types, say text, floats, and dates, where each type of feature requires separate preprocessing or feature extraction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use subset of our data to illustrate few more points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  \\\n",
       "0        1          60       RL         65.0     8450   Pave      Reg   \n",
       "1        2          20       RL         80.0     9600   Pave      Reg   \n",
       "2        3          60       RL         68.0    11250   Pave      IR1   \n",
       "3        4          70       RL         60.0     9550   Pave      IR1   \n",
       "4        5          60       RL         84.0    14260   Pave      IR1   \n",
       "...    ...         ...      ...          ...      ...    ...      ...   \n",
       "1455  1456          60       RL         62.0     7917   Pave      Reg   \n",
       "1456  1457          20       RL         85.0    13175   Pave      Reg   \n",
       "1457  1458          70       RL         66.0     9042   Pave      Reg   \n",
       "1458  1459          20       RL         68.0     9717   Pave      Reg   \n",
       "1459  1460          20       RL         75.0     9937   Pave      Reg   \n",
       "\n",
       "     LandContour Utilities  \n",
       "0            Lvl    AllPub  \n",
       "1            Lvl    AllPub  \n",
       "2            Lvl    AllPub  \n",
       "3            Lvl    AllPub  \n",
       "4            Lvl    AllPub  \n",
       "...          ...       ...  \n",
       "1455         Lvl    AllPub  \n",
       "1456         Lvl    AllPub  \n",
       "1457         Lvl    AllPub  \n",
       "1458         Lvl    AllPub  \n",
       "1459         Lvl    AllPub  \n",
       "\n",
       "[1460 rows x 9 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=train_data.iloc[:,0:10].drop(['Alley'],axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'LotShape', 'LandContour', 'Utilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data, we might want to encode the 'street' column as a categorical variable using preprocessing.\n",
    "\n",
    "As we might use multiple feature extraction methods on the same column, we give each transformer a unique name, say 'street_category'. \n",
    "\n",
    "By default, the remaining rating columns are ignored (remainder='drop').\n",
    "\n",
    "We can keep the remaining rating columns by setting remainder='passthrough' also the remainder parameter can be set to an estimator to transform the remaining rating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "                  transformer_weights=None,\n",
       "                  transformers=[('street_category',\n",
       "                                 OneHotEncoder(categories='auto', drop=None,\n",
       "                                               dtype='int',\n",
       "                                               handle_unknown='error',\n",
       "                                               sparse=True),\n",
       "                                 ['Street'])],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "     [('street_category', OneHotEncoder(dtype='int'),['Street'])],\n",
    "     remainder='drop')\n",
    "\n",
    "column_trans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['street_category__x0_Grvl', 'street_category__x0_Pave']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_trans.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The make_column_selector is used to select columns based on data type or column name. Lets use OneHotEncoder on categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.73086488,  0.07337496, -0.20803433, -0.20714171,  0.        ,\n",
       "         1.        ],\n",
       "       [-1.7284922 , -0.87256276,  0.40989452, -0.09188637,  0.        ,\n",
       "         1.        ],\n",
       "       [-1.72611953,  0.07337496, -0.08444856,  0.07347998,  0.        ,\n",
       "         1.        ],\n",
       "       ...,\n",
       "       [ 1.72611953,  0.30985939, -0.16683907, -0.14781027,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.7284922 , -0.87256276, -0.08444856, -0.08016039,  0.        ,\n",
       "         1.        ],\n",
       "       [ 1.73086488, -0.87256276,  0.20391824, -0.05811155,  0.        ,\n",
       "         1.        ]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_selector\n",
    "ct = ColumnTransformer([\n",
    "       ('scale', StandardScaler(),\n",
    "       make_column_selector(dtype_include=np.number)),\n",
    "       ('ohe',OneHotEncoder(),\n",
    "       make_column_selector(pattern='Street', dtype_include=object))])\n",
    "ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you will use LabelEncoder() in place of OneHotEncoder() you will run into this error \n",
    "\n",
    "fit_transform() takes 2 positional arguments but 3 were given\n",
    "\n",
    "Read here for workaround\n",
    "https://stackoverflow.com/questions/46162855/fit-transform-takes-2-positional-arguments-but-3-were-given-with-labelbinarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't I told you that that we can use custom functionalities! Let's Change the labelencoder is implemented so that we can use it in our pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to add some custom functionallity to your pipeline it will be basically two things either you will want to do some transformation which is not present in sklearn(or present but not in suitable format) or some estimator!\n",
    "\n",
    "You should have obsereved by now that  we have to make a object of every transformer or estimator before calling functions over it.For example\n",
    "\n",
    ">lr=LinearRegression()\n",
    "\n",
    ">lr.fit(data)\n",
    "\n",
    "Pipelines also work in the same way so we will need to implememnt our functionallities as Classes!But you dont have to do everything from scratch Scikit-Learn got you covered.\n",
    "\n",
    "Writing custom functionallity in SKlearn depends upon the inheritence of two classes:\n",
    "\n",
    "\n",
    "- class sklearn.base.TransformerMixi:\n",
    "Mixin class for all transformers in scikit-learn.This is the base class for writting all kinds of transformation you want ! That is all other classes will derive it as a parent class.\n",
    "\n",
    "- class sklearn.base.BaseEstimator:\n",
    "Base class for all estimators in scikit-learn and it for writting estimators!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is your custom functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder(BaseEstimator,TransformerMixin):\n",
    "    \n",
    "    def __init__(self, columns = None):\n",
    "        self.columns = columns # list of column to encode\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        \n",
    "        output = X.copy()\n",
    "        \n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname, col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        \n",
    "        return output\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-583a858e48e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m sk_pipe = Pipeline([ (\"mssing\", SimpleImputer(Strategy='most_frequent'))\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MLCLE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiColumnLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     (\"lr\", LinearRegression())])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "sk_pipe = Pipeline([ (\"mssing\", SimpleImputer(Strategy='most_frequent'))\n",
    "                    ,(\"MLCLE\", MultiColumnLabelEncoder()), \n",
    "                    (\"lr\", LinearRegression())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1460, 506]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-fa33382348d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msk_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 492\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1460, 506]"
     ]
    }
   ],
   "source": [
    "sk_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also exploit featureUnion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit created a FunctionTransformer as part of the preprocessing class. It can be used in a similar manner as above but with less flexibility. If the input/output of the function is configured properly, the transformer can implement the fit/transform/fit_transform methods for the function and thus allow it to be used in the scikit pipeline.\n",
    "\n",
    "For example, if the input to a pipeline is a series, the transformer would be as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_func(input_series):\n",
    "    return output_series\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "name_transformer = FunctionTransformer(trans_func)\n",
    "\n",
    "sk_pipe = Pipeline([(\"trans\", name_transformer), (\"lr\", LinearRegression())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('trans',\n",
       "                 FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                                     func=<function trans_func at 0x7f4ed9cbc1e0>,\n",
       "                                     inv_kw_args=None, inverse_func=None,\n",
       "                                     kw_args=None, validate=False)),\n",
       "                ('lr',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put this all concept together and create simple pipleine!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before jumping to pipelines a few thigs still needed to be taken careof: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../hitchhikersGuideToMachineLearning/home-data-for-ml-course/train.csv' , index_col ='Id') \n",
    "X_test_full = pd.read_csv('../hitchhikersGuideToMachineLearning/home-data-for-ml-course/test.csv', index_col='Id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id\n",
       "1       CollgCr\n",
       "2       Veenker\n",
       "3       CollgCr\n",
       "4       Crawfor\n",
       "5       NoRidge\n",
       "         ...   \n",
       "1456    Gilbert\n",
       "1457     NWAmes\n",
       "1458    Crawfor\n",
       "1459      NAmes\n",
       "1460    Edwards\n",
       "Name: Neighborhood, Length: 1460, dtype: object"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_full['Neighborhood'].unique()\n",
    "train_data['Neighborhood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = train_data.SalePrice\n",
    "train_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train_full, X_valid_full, y_train, y_valid = train_test_split(train_data, y, train_size=0.8, test_size=0.2,\n",
    "                                                                random_state=0)\n",
    "\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary) for one hot encoding\n",
    "categorical_cols_type1 = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() <= 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select categorical columns with high cardinality (convenient but arbitrary) for one hot encoding\n",
    "categorical_cols_type2 = [cname for cname in X_train_full.columns if\n",
    "                    X_train_full[cname].nunique() > 10 and \n",
    "                    X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols_type1+categorical_cols_type2 + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_valid = X_valid_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelOneHotEncoder():\n",
    "    def __init__(self):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.le = LabelEncoder()\n",
    "    def fit_transform(self, x):\n",
    "        features = self.le.fit_transform( x)\n",
    "        return self.ohe.fit_transform( features.reshape(-1,1))\n",
    "    def transform( self, x):\n",
    "        return self.ohe.transform( self.la.transform( x.reshape(-1,1)))\n",
    "    def inverse_tranform( self, x):\n",
    "        return self.le.inverse_transform( self.ohe.inverse_tranform( x))\n",
    "    def inverse_labels( self, x):\n",
    "        return self.le.inverse_transform( x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModifiedLabelEncoder(LabelEncoder):\n",
    "\n",
    "    def fit_transform(self, y, *args, **kwargs):\n",
    "        return super().fit_transform(y).reshape(-1, 1)\n",
    "\n",
    "    def transform(self, y, *args, **kwargs):\n",
    "        return super().transform(y).reshape(-1, 1)\n",
    "\n",
    "\n",
    "pipe = Pipeline([(\"le\", ModifiedLabelEncoder()), (\"ohe\", OneHotEncoder())])\n",
    "pipe.fit_transform(['dog', 'cat', 'dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1168x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1168 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit_transform(X_train[\"Street\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('model', model)\n",
    "                     ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "print('MAE:', mean_absolute_error(y_valid, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for numerical data\n",
    "numerical_transformer1 = SimpleImputer(strategy='constant') # Your code here\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer1 = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "]) # Your code here\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer1, numerical_cols),\n",
    "        ('cat', categorical_transformer1, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define model\n",
    "model1 = RandomForestRegressor(n_estimators=150, random_state=0)\n",
    " # Your code here\n",
    "\n",
    "# Check your answer\n",
    "step_1.a.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor1),\n",
    "                              ('model', model1)\n",
    "                             ])\n",
    "\n",
    "# Preprocessing of training data, fit model \n",
    "my_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print('MAE:', score)\n",
    "\n",
    "# Check your answer\n",
    "step_1.b.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datanami.com/2018/09/05/how-to-build-a-better-machine-learning-pipeline/\n",
    "https://www.analyticsvidhya.com/blog/2020/01/build-your-first-machine-learning-pipeline-using-scikit-learn/\n",
    "https://cloud.google.com/ai-platform/prediction/docs/custom-pipeline\n",
    "https://stackoverflow.com/questions/31259891/put-customized-functions-in-sklearn-pipeline\n",
    "\n",
    "http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n",
    "https://g-stat.com/using-custom-transformers-in-your-machine-learning-pipelines-with-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
